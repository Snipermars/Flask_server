# -*- coding: utf-8 -*-
from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.mllib.feature import *
from pyspark.mllib.regression import LabeledPoint
from math import exp
import numpy as np

#################### settings ####################
conf = SparkConf()
conf = conf.set("spark.executor.memory", "60G").set("spark.cores.max", "48").set("spark.executor.cores", "8")
conf = conf.setMaster("spark://jtcrtppra22:7077").setAppName("mixc_mem_summary")

golden_copy_data_dir = '/user/data/ProjectVenus/MixcClustering/Data/GoldenCopy/'
mem_summary_data_dir = '/user/data/ProjectVenus/MixcClustering/Data/MemSummary/'

#td_mem_info_dir = "/app/hive/warehouse/kudu_zdsy_imp_back.db/td_member_info/"
#mobile_dir = "/tmp/t/"
#################### settings ####################


# create a SparkSession
spark = SparkSession.builder.config(conf=conf).getOrCreate()

# load data
golden_copy_df = spark.read.format('csv').load(golden_copy_data_dir + '*.csv', header=True).dropDuplicates().filter('m_id != "none"')
td_mem_info_df = spark.read.format('parquet').load(td_mem_info_dir + '*.parq').dropDuplicates()
#mobile_df = spark.read.format('csv').load(mobile_dir + 'mobile.csv',header=True).dropDuplicates()

# add the column gender and Guerlain_code

td_mem_info_df.createOrReplaceTempView("td_mem_info_df")
#mobile_df.createOrReplaceTempView("mobile_df")

mem_df = spark.sql(
    " select trim(cast(member_id as string)) as m_id, case when sex = '男' then '1' else '0' end as gender_id from td_mem_info_df ").dropDuplicates()
	
# convert datatypes
golden_copy_df2 = golden_copy_df \
    .withColumn('actual_pay_cash', col('actual_pay_cash').cast('float')) \
    .withColumn('discount_value', col('discount_value').cast('float'))

# convert date into day of week
golden_copy_df3_bak = golden_copy_df2 \
    .select(map(lambda x: x.name, golden_copy_df2.schema)
            + [date_format(golden_copy_df2.trans_date, 'E').alias('day_of_week')]
            + [concat(year(golden_copy_df2.trans_date), weekofyear(golden_copy_df2.trans_date)).alias('week_of_year')])

golden_copy_df3_bak2 = golden_copy_df3_bak.select('*', split(golden_copy_df3_bak.trans_date, '-').alias('trans_date_array'))\
	.filter('int(trans_date_array[0]) >= 2016').drop('trans_date_array')\
	.dropDuplicates()
	
golden_copy_df3_bak2.createOrReplaceTempView('golden_copy_df3_bak2')
	
	
golden_copy_df3_bak2.select('m_id', 'store_rank', 'store_no').registerTempTable('golden_copy_df4')
golden_copy_df4_bak = spark.sql("with tmp2 as (with tmp1 as ("
	" Select *, case when store_rank = '奢侈' or store_rank = '轻奢' then '1' else '0' end as consum_type"
	" ,case when store_no = 'B0110N02' then '1' else '0' end as Guerlian_is "
	" from golden_copy_df4 gd4)"
	" select t1.*, Row_number() over (partition by t1.m_id order by t1.consum_type desc) as cnt from tmp1 t1)"
	" select t2.m_id, t2.consum_type, t2.Guerlian_is from tmp2 t2 where cnt = 1"
)
	
golden_copy_df4 = golden_copy_df4_bak\
	.join(mem_df, ['m_id'], 'left')\
	.filter('gender_id != "null"')
	
golden_copy_df3 = golden_copy_df3_bak2.filter('siebel_code = "1402A001"')
	


#################### compute summary variables ####################
# compute total spend
total_spend_df = golden_copy_df3.groupBy('m_id').sum('actual_pay_cash') \
    .withColumnRenamed('sum(actual_pay_cash)', 'total_spend')

# compute total order number
total_order_df = golden_copy_df3.groupBy('m_id').count() \
    .withColumnRenamed('count', 'total_order_cnt')

# compute spend per order
spend_per_order_df1 = total_spend_df.join(total_order_df, ['m_id'], 'inner')
spend_per_order_df2 = spend_per_order_df1 \
    .withColumn('spend_per_order', col('total_spend') / col('total_order_cnt'))
#################### compute summary variables ####################


#################### compute dow spend percentage ####################
dow_spend_df1 = golden_copy_df3.groupBy('m_id', 'day_of_week').count() \
    .withColumnRenamed('count', 'actual_pay_cash_per_dow')
	
dow_spend_df2 = dow_spend_df1.join(total_order_df, ['m_id'], 'inner')

dow_spend_df3 = dow_spend_df2 \
    .withColumn('dow_spend_perct', col('actual_pay_cash_per_dow') / col('total_order_cnt'))

dow_spend_df4 = dow_spend_df3.groupBy('m_id').pivot('day_of_week').sum('dow_spend_perct').na.fill(0.0)
#################### compute dow spend percentage ####################

'''
#################### compute store spend percentage ####################
store_spend_df1 = golden_copy_df3.groupBy('m_id', 'shop_id').count() \
    .withColumnRenamed('count', 'spend_per_store')

store_spend_df2 = store_spend_df1.join(total_order_df, ['m_id'], 'inner')

store_spend_df3 = store_spend_df2 \
    .withColumn('store_spend_perct', store_spend_df2.spend_per_store / store_spend_df2.total_order_cnt)

store_spend_df4 = store_spend_df3.groupBy('m_id').pivot('shop_id').sum('store_spend_perct').na.fill(0.0)
#################### compute store spend percentage ####################
'''

#################### compute sub_industry spend order percentage ####################
indus_order_df1_bak = golden_copy_df3.groupBy('m_id', 'sub_indus_type_id').count() \
    .withColumnRenamed('count', 'order_per_sub_indus')
	
indus_order_df1_back = indus_order_df1_bak.join(golden_copy_df3, ['m_id', 'sub_indus_type_id'],  'left')\
	.withColumnRenamed('sub_indus_type_id', 'sub_indus_type')\
	.groupBy('m_id', 'sub_indus_type', 'order_per_sub_indus', 'store_rank').sum('actual_pay_cash')\
	.withColumnRenamed('sum(actual_pay_cash)', 'actual_pay_cash')
indus_order_df1_back.createOrReplaceTempView("indus_order_df1")
indus_order_df1 = spark.sql("select *, CASE WHEN iod.store_rank = '奢侈' THEN concat(sub_indus_type, '_L')"
				" WHEN iod.store_rank = '轻奢' THEN concat(sub_indus_type, '_M')"
				" WHEN iod.store_rank = '大众' THEN concat(sub_indus_type, '_S')"
				" ELSE concat(sub_indus_type, '_N') END AS sub_indus_type_id FROM indus_order_df1 iod")

indus_order_df2 = indus_order_df1.join(total_order_df, ['m_id'], 'inner').filter('sub_indus_type_id != "null"')

indus_order_df3 = indus_order_df2 \
    .withColumn('indus_order_perct', col('order_per_sub_indus') / col('total_order_cnt'))\
	.drop('sub_indus_type')
indus_order_df3.createOrReplaceTempView("indus_order_df3")
indus_order_df4 = spark.sql("select *, concat('M_sindus_',sub_indus_type_id) as sub_indus_type from indus_order_df3")
indus_order_df5 = indus_order_df4.groupBy('m_id').pivot('sub_indus_type').sum('indus_order_perct').na.fill(0.0)
	
indus_order_df8 = indus_order_df1.groupBy('m_id', 'sub_indus_type_id').sum('actual_pay_cash')\
	.withColumnRenamed('sum(actual_pay_cash)', 'spend_per_sub_indus')
indus_order_df8.createOrReplaceTempView("indus_order_df8")
indus_order_df9 = spark.sql("select *, concat('M_sindusCash_',sub_indus_type_id) as sub_indus_spend_per from indus_order_df8")
indus_order_df11 = indus_order_df9.groupBy('m_id').pivot('sub_indus_spend_per').sum('spend_per_sub_indus').na.fill(0.0)

indus_order_df6 = indus_order_df1.join(indus_order_df8, ['m_id', 'sub_indus_type_id'], 'left')
indus_order_df7 = indus_order_df6.withColumn('indus_order_per_spend', col('spend_per_sub_indus') / col('order_per_sub_indus'))
indus_order_df7_e = indus_order_df7.select('*', log('indus_order_per_spend').alias('e'))

#################### nomalization #######################
r1 = indus_order_df7_e.rdd
gd_df7_e_LP_rdd = r1.map(lambda x: LabeledPoint(x[-1], map(lambda y: float(y), [x[-1]])))
label_rdd = r1.map(lambda x: x[:-1])
feature_rdd = gd_df7_e_LP_rdd.map(lambda x: x.features)
scaler = StandardScaler(withMean=True, withStd=True)
feature_scaled_rdd = scaler.fit(feature_rdd).transform(feature_rdd)
r2 = label_rdd.zip(feature_scaled_rdd)
r3 = r2.map(lambda (x,y):map(lambda z: z,x)+[float(y[-1])])
indus_order_df13 = spark.createDataFrame(r3, indus_order_df7.columns)\
	.withColumnRenamed('_9', 'e_indus_order_per_spend')
	
indus_order_df13.createOrReplaceTempView('indus_order_df13')
indus_order_df10 = spark.sql("select *, concat('M_sindusPerOdr_',sub_indus_type_id) as sub_indus_spend_order_per from indus_order_df13")\
    .drop('sub_indus_type_id')
indus_order_df12 = indus_order_df10.groupBy('m_id').pivot('sub_indus_spend_order_per').sum('e_indus_order_per_spend').na.fill(0.0)
#################### nomalization #######################	
indus_order_df = indus_order_df1.select('m_id').dropDuplicates()\
	.join(indus_order_df5, ['m_id'], 'inner')\
	.join(indus_order_df11, ['m_id'], 'inner')\
	.join(indus_order_df12, ['m_id'], 'inner')	

#################### compute industry spend percentage ####################


#################### compute dow variability ####################
golden_copy_df3_1 = golden_copy_df3.select('m_id', 'week_of_year').dropDuplicates().groupBy('m_id').count()

dow_var_df1 = golden_copy_df3.groupBy('m_id', 'week_of_year', 'day_of_week').count() \
    .withColumnRenamed('count', 'spend_per_dow_woy')

total_order_per_wk_df = golden_copy_df3.groupBy('m_id', 'week_of_year').count() \
    .withColumnRenamed('count', 'spend_per_week')

dow_var_df2 = dow_var_df1.join(total_order_per_wk_df, ['m_id', 'week_of_year'], 'inner')

dow_var_df3 = dow_var_df2 \
    .withColumn('order_perct_per_dow_woy', dow_var_df2.spend_per_dow_woy / dow_var_df2.spend_per_week) \
    .drop('spend_per_dow_woy') \
    .drop('spend_per_week') \
    .join(golden_copy_df3_1, ['m_id'], 'inner')

dow_var_r1 = dow_var_df3.rdd.map(lambda x: ((x[0], x[2]), [x[3], x[4]]))

dow_var_r2 = dow_var_r1.groupByKey().mapValues(lambda x: list(x))


def col_sd(data_list):
    cnt = data_list[0][1]
    if cnt == 1:
        return 0.0
    data_list2 = map(lambda x: x[0], data_list)
    l = len(data_list2)
    data_list3 = data_list2 + [0.0] * (cnt - l)
    return np.std(data_list3)


dow_var_r3 = dow_var_r2.mapValues(col_sd).map(lambda (x, y): (x[0], y))

dow_var_r4 = dow_var_r3.groupByKey().mapValues(lambda x: list(x)) \
    .mapValues(lambda x: float(np.sum(x) / 7.0))

dow_var_df4 = spark.createDataFrame(dow_var_r4, ['m_id', 'dow_variability'])
#################### compute dow variability ####################


"""
#################### compute store variability ####################
df3_2 = golden_copy_df3.select('m_id', 'trans_date').dropDuplicates().groupBy('m_id').count()

store_var_df1 = golden_copy_df3.groupBy('m_id', 'trans_date', 'shop_id').sum('actual_pay_cash') \
    .withColumnRenamed('sum(actual_pay_cash)', 'spend_per_visit_store')

total_spend_per_visit_df = golden_copy_df3.groupBy('m_id', 'trans_date').sum('actual_pay_cash') \
    .withColumnRenamed('sum(actual_pay_cash)', 'spend_per_visit')

store_var_df2 = store_var_df1.join(total_spend_per_visit_df, ['m_id', 'trans_date'], 'inner')

store_var_df3 = store_var_df2 \
    .withColumn('spend_per_store_perct', store_var_df2.spend_per_visit_store / store_var_df2.spend_per_visit) \
    .drop('spend_per_visit_store') \
    .drop('spend_per_visit') \
    .join(df3_2, ['m_id'], 'inner')

store_var_r1 = store_var_df3.rdd.map(lambda x: ((x[0], x[2]), [x[3], x[4]]))

store_var_r2 = store_var_r1.groupByKey().map(lambda x: (x[0], list(x[1])))

store_var_r3 = store_var_r2.mapValues(col_sd).map(lambda (x, y): (x[0], y))

store_num = df3.select('shop_id').dropDuplicates().count()

store_var_r4 = store_var_r3.groupByKey().map(lambda x: (x[0], list(x[1]))) \
    .mapValues(lambda x: float(np.sum(x) / store_num))

store_var_df4 = spark.createDataFrame(store_var_r4, ['m_id', 'store_variability'])
#################### compute store variability ####################
"""

#################### compute industry variability ####################
golden_copy_df3_2 = golden_copy_df3.select('m_id', 'order_id').dropDuplicates().groupBy('m_id').count()

indus_var_df1 = golden_copy_df3.groupBy('m_id', 'order_id', 'sub_indus_type_id').sum('actual_pay_cash') \
    .withColumnRenamed('sum(actual_pay_cash)', 'spend_per_visit_indus')

total_spend_per_visit_df = golden_copy_df3.groupBy('m_id', 'order_id').sum('actual_pay_cash') \
    .withColumnRenamed('sum(actual_pay_cash)', 'spend_per_indus')

indus_var_df2 = indus_var_df1.join(total_spend_per_visit_df, ['m_id', 'order_id'], 'inner')

indus_var_df3 = indus_var_df2 \
    .withColumn('spend_per_indus_perct', indus_var_df2.spend_per_visit_indus / indus_var_df2.spend_per_indus) \
    .drop('spend_per_visit_indus') \
    .drop('spend_per_indus') \
    .join(golden_copy_df3_2, ['m_id'], 'inner')

indus_var_r1 = indus_var_df3.rdd.map(lambda x: ((x[0], x[2]), [x[3], x[4]]))

indus_var_r2 = indus_var_r1.groupByKey().map(lambda x: (x[0], list(x[1])))

indus_var_r3 = indus_var_r2.mapValues(col_sd).map(lambda (x, y): (x[0], y))

indus_num = golden_copy_df3.select('indus_type_id').dropDuplicates().count()

indus_var_r4 = indus_var_r3.groupByKey().map(lambda x: (x[0], list(x[1]))) \
    .mapValues(lambda x: float(np.sum(x) / indus_num))

indus_var_df4 = spark.createDataFrame(indus_var_r4, ['m_id', 'indus_variability'])
#################### compute industry variability ####################


#################### combine variables together ####################
mem_summary_df_bak = spend_per_order_df2 \
    .join(dow_spend_df4, 'm_id', 'inner') \
    .join(indus_order_df, 'm_id', 'inner') \
    .join(dow_var_df4, 'm_id', 'inner') \
    .join(indus_var_df4, 'm_id', 'inner')\
	.join(golden_copy_df4, 'm_id', 'inner')
	
mem_summary_df = mem_summary_df_bak\
	.withColumnRenamed('total_spend', 'M_TotalCashAmt')\
	.withColumnRenamed('total_order_cnt', 'M_CashOdrCnt')\
	.withColumnRenamed('spend_per_order', 'M_CashPerOdr')\
	.withColumnRenamed('Fri', 'M_FriOdrPerct')\
	.withColumnRenamed('Mon', 'M_MonOdrPerct')\
	.withColumnRenamed('Sat', 'M_SatOdrPerct')\
	.withColumnRenamed('Sun', 'M_SunOdrPerct')\
	.withColumnRenamed('Thu', 'M_ThuOdrPerct')\
	.withColumnRenamed('Tue', 'M_TueOdrPerct')\
	.withColumnRenamed('Wed', 'M_WedOdrPerct')\
	.withColumnRenamed('dow_variability', 'M_DoWAmtPctVar')\
	.withColumnRenamed('indus_variability', 'M_IndusAmtPctVar')\
	.withColumnRenamed('Guerlian_is', 'M_GuerlianIs')\
	.withColumnRenamed('consum_type', 'M_ConsumType')\
	.withColumnRenamed('gender_id', 'M_Sex')

	
#################### combine variables together ####################

# save to HDFS file
mem_summary_df.dropDuplicates().write.format('csv').save(mem_summary_data_dir, mode='overwrite', header=True)
print("Mixc Golden Copy saved to %s" % mem_summary_data_dir)

spark.stop()
